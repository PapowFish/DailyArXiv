---
title: Latest 15 Papers - November 13, 2025
labels: documentation
---
**Please check the [Github](https://github.com/PapowFish/DailyArXiv) page for a better reading experience and more papers.**

## Video Retrieval
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Video Editing for Video Retrieval](https://arxiv.org/pdf/2402.02335v2)** | 2024-09-10 |  |
| **[Dialogue-to-Video Retrieval](https://arxiv.org/pdf/2303.16761v1)** | 2023-03-30 |  |
| **[Content based video retrieval](https://arxiv.org/pdf/1211.4683v1)** | 2012-11-21 |  |
| **[Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval](https://arxiv.org/pdf/2508.11313v1)** | 2025-08-18 | <details><summary>Accep...</summary><p>Accepted by IJCAI 2025</p></details> |
| **[Multi-Query Video Retrieval](https://arxiv.org/pdf/2201.03639v2)** | 2022-07-22 | ECCV 2022 |
| **[On Semantic Similarity in Video Retrieval](https://arxiv.org/pdf/2103.10095v1)** | 2021-03-19 | <details><summary>Accep...</summary><p>Accepted in CVPR 2021. Project Page: https://mwray.github.io/SSVR/</p></details> |
| **[Interactive Video Retrieval with Dialog](https://arxiv.org/pdf/1905.02442v1)** | 2019-05-08 |  |
| **[Retrieval-Augmented Egocentric Video Captioning](https://arxiv.org/pdf/2401.00789v4)** | 2024-06-21 | <details><summary>CVPR ...</summary><p>CVPR 2024. Project page is available at: https://jazzcharles.github.io/Egoinstructor/</p></details> |
| **[Self-supervised Video Retrieval Transformer Network](https://arxiv.org/pdf/2104.07993v1)** | 2021-04-19 |  |
| **[Content based video retrieval systems](https://arxiv.org/pdf/1205.1641v1)** | 2012-05-09 | 18 Pages |
| **[VVS: Video-to-Video Retrieval with Irrelevant Frame Suppression](https://arxiv.org/pdf/2303.08906v2)** | 2023-12-20 | AAAI-24 |
| **[T2VIndexer: A Generative Video Indexer for Efficient Text-Video Retrieval](https://arxiv.org/pdf/2408.11432v1)** | 2024-08-22 |  |
| **[Multimodal Lengthy Videos Retrieval Framework and Evaluation Metric](https://arxiv.org/pdf/2504.04572v1)** | 2025-04-08 |  |
| **[Learning to Retrieve Videos by Asking Questions](https://arxiv.org/pdf/2205.05739v3)** | 2022-07-19 |  |
| **[FIVR: Fine-grained Incident Video Retrieval](https://arxiv.org/pdf/1809.04094v2)** | 2019-03-26 |  |

## Multimodal Retrieval
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs](https://arxiv.org/pdf/2411.02571v2)** | 2025-02-25 | <details><summary>Accep...</summary><p>Accepted at ICLR 2025. We release the model weights at: https://huggingface.co/nvidia/MM-Embed</p></details> |
| **[A Survey of Multimodal Retrieval-Augmented Generation](https://arxiv.org/pdf/2504.08748v1)** | 2025-04-15 |  |
| **[A Survey of Multimodal Composite Editing and Retrieval](https://arxiv.org/pdf/2409.05405v2)** | 2024-09-12 | <details><summary>20 pa...</summary><p>20 pages, 3 figures, and 11 tables</p></details> |
| **[Mr. Right: Multimodal Retrieval on Representation of ImaGe witH Text](https://arxiv.org/pdf/2209.13764v1)** | 2022-09-29 | <details><summary>Datas...</summary><p>Dataset available at https://github.com/hsiehjackson/Mr.Right</p></details> |
| **[M3Retrieve: Benchmarking Multimodal Retrieval for Medicine](https://arxiv.org/pdf/2510.06888v1)** | 2025-10-09 | EMNLP Mains 2025 |
| **[Universal Retrieval for Multimodal Trajectory Modeling](https://arxiv.org/pdf/2506.22056v1)** | 2025-06-30 | <details><summary>18 pa...</summary><p>18 pages, 3 figures, accepted by Workshop on Computer-use Agents @ ICML 2025</p></details> |
| **[Self-adaptive Multimodal Retrieval-Augmented Generation](https://arxiv.org/pdf/2410.11321v1)** | 2024-10-16 |  |
| **[MMDocIR: Benchmarking Multimodal Retrieval for Long Documents](https://arxiv.org/pdf/2501.08828v3)** | 2025-11-10 | <details><summary>Paper...</summary><p>Paper accepted to EMNLP-2025(Main)</p></details> |
| **[Multimodal semantic retrieval for product search](https://arxiv.org/pdf/2501.07365v3)** | 2025-02-18 | <details><summary>Accep...</summary><p>Accepted at EReL@MIR WWW 2025</p></details> |
| **[Retrieval-Augmented Multimodal Language Modeling](https://arxiv.org/pdf/2211.12561v2)** | 2023-06-07 | <details><summary>Publi...</summary><p>Published at ICML 2023. Blog post available at https://cs.stanford.edu/~myasu/blog/racm3/</p></details> |
| **[Probabilistic Compositional Embeddings for Multimodal Image Retrieval](https://arxiv.org/pdf/2204.05845v1)** | 2022-04-13 | <details><summary>CVPR2...</summary><p>CVPR2022 MULA workshop</p></details> |
| **[UniIR: Training and Benchmarking Universal Multimodal Information Retrievers](https://arxiv.org/pdf/2311.17136v1)** | 2023-11-30 | <details><summary>Our c...</summary><p>Our code and dataset are available on this project page: https://tiger-ai-lab.github.io/UniIR/</p></details> |
| **[MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering](https://arxiv.org/pdf/2408.08521v2)** | 2025-02-10 | <details><summary>Accep...</summary><p>Accepted at COLING 2025</p></details> |
| **[Progressive Multimodal Reasoning via Active Retrieval](https://arxiv.org/pdf/2412.14835v1)** | 2024-12-20 | Working in progress |
| **[Generalized Contrastive Learning for Universal Multimodal Retrieval](https://arxiv.org/pdf/2509.25638v1)** | 2025-10-01 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2025</p></details> |

